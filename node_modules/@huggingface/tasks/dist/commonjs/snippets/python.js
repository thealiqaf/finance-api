"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.pythonSnippets = exports.snippetDocumentQuestionAnswering = exports.snippetTextToAudio = exports.snippetTabular = exports.snippetTextToImage = exports.snippetFile = exports.snippetBasic = exports.snippetZeroShotImageClassification = exports.snippetZeroShotClassification = exports.snippetConversational = void 0;
exports.getPythonInferenceSnippet = getPythonInferenceSnippet;
const inference_providers_js_1 = require("../inference-providers.js");
const common_js_1 = require("./common.js");
const inputs_js_1 = require("./inputs.js");
const HFH_INFERENCE_CLIENT_METHODS = {
    "audio-classification": "audio_classification",
    "audio-to-audio": "audio_to_audio",
    "automatic-speech-recognition": "automatic_speech_recognition",
    "text-to-speech": "text_to_speech",
    "image-classification": "image_classification",
    "image-segmentation": "image_segmentation",
    "image-to-image": "image_to_image",
    "image-to-text": "image_to_text",
    "object-detection": "object_detection",
    "text-to-image": "text_to_image",
    "text-to-video": "text_to_video",
    "zero-shot-image-classification": "zero_shot_image_classification",
    "document-question-answering": "document_question_answering",
    "visual-question-answering": "visual_question_answering",
    "feature-extraction": "feature_extraction",
    "fill-mask": "fill_mask",
    "question-answering": "question_answering",
    "sentence-similarity": "sentence_similarity",
    summarization: "summarization",
    "table-question-answering": "table_question_answering",
    "text-classification": "text_classification",
    "text-generation": "text_generation",
    "token-classification": "token_classification",
    translation: "translation",
    "zero-shot-classification": "zero_shot_classification",
    "tabular-classification": "tabular_classification",
    "tabular-regression": "tabular_regression",
};
const snippetImportInferenceClient = (accessToken, provider) => `\
from huggingface_hub import InferenceClient

client = InferenceClient(
	provider="${provider}",
	api_key="${accessToken || "{API_TOKEN}"}"
)`;
const snippetConversational = (model, accessToken, provider, opts) => {
    const streaming = opts?.streaming ?? true;
    const exampleMessages = (0, inputs_js_1.getModelInputSnippet)(model);
    const messages = opts?.messages ?? exampleMessages;
    const messagesStr = (0, common_js_1.stringifyMessages)(messages, { attributeKeyQuotes: true });
    const config = {
        ...(opts?.temperature ? { temperature: opts.temperature } : undefined),
        max_tokens: opts?.max_tokens ?? 500,
        ...(opts?.top_p ? { top_p: opts.top_p } : undefined),
    };
    const configStr = (0, common_js_1.stringifyGenerationConfig)(config, {
        indent: "\n\t",
        attributeValueConnector: "=",
    });
    if (streaming) {
        return [
            {
                client: "huggingface_hub",
                content: `\
${snippetImportInferenceClient(accessToken, provider)}

messages = ${messagesStr}

stream = client.chat.completions.create(
	model="${model.id}", 
	messages=messages, 
	${configStr}
	stream=True
)

for chunk in stream:
    print(chunk.choices[0].delta.content, end="")`,
            },
            {
                client: "openai",
                content: `\
from openai import OpenAI

client = OpenAI(
	base_url="${(0, inference_providers_js_1.openAIbaseUrl)(provider)}",
	api_key="${accessToken || "{API_TOKEN}"}"
)

messages = ${messagesStr}

stream = client.chat.completions.create(
    model="${model.id}", 
	messages=messages, 
	${configStr}
	stream=True
)

for chunk in stream:
	print(chunk.choices[0].delta.content, end="")`,
            },
        ];
    }
    else {
        return [
            {
                client: "huggingface_hub",
                content: `\
${snippetImportInferenceClient(accessToken, provider)}

messages = ${messagesStr}

completion = client.chat.completions.create(
    model="${model.id}", 
	messages=messages, 
	${configStr}
)

print(completion.choices[0].message)`,
            },
            {
                client: "openai",
                content: `\
from openai import OpenAI

client = OpenAI(
	base_url="${(0, inference_providers_js_1.openAIbaseUrl)(provider)}",
	api_key="${accessToken || "{API_TOKEN}"}"
)

messages = ${messagesStr}

completion = client.chat.completions.create(
	model="${model.id}", 
	messages=messages, 
	${configStr}
)

print(completion.choices[0].message)`,
            },
        ];
    }
};
exports.snippetConversational = snippetConversational;
const snippetZeroShotClassification = (model) => {
    return [
        {
            client: "requests",
            content: `\
def query(payload):
	response = requests.post(API_URL, headers=headers, json=payload)
	return response.json()

output = query({
    "inputs": ${(0, inputs_js_1.getModelInputSnippet)(model)},
    "parameters": {"candidate_labels": ["refund", "legal", "faq"]},
})`,
        },
    ];
};
exports.snippetZeroShotClassification = snippetZeroShotClassification;
const snippetZeroShotImageClassification = (model) => {
    return [
        {
            client: "requests",
            content: `\
def query(data):
	with open(data["image_path"], "rb") as f:
		img = f.read()
	payload={
		"parameters": data["parameters"],
		"inputs": base64.b64encode(img).decode("utf-8")
	}
	response = requests.post(API_URL, headers=headers, json=payload)
	return response.json()

output = query({
	"image_path": ${(0, inputs_js_1.getModelInputSnippet)(model)},
	"parameters": {"candidate_labels": ["cat", "dog", "llama"]},
})`,
        },
    ];
};
exports.snippetZeroShotImageClassification = snippetZeroShotImageClassification;
const snippetBasic = (model, accessToken, provider) => {
    return [
        ...(model.pipeline_tag && model.pipeline_tag in HFH_INFERENCE_CLIENT_METHODS
            ? [
                {
                    client: "huggingface_hub",
                    content: `\
${snippetImportInferenceClient(accessToken, provider)}

result = client.${HFH_INFERENCE_CLIENT_METHODS[model.pipeline_tag]}(
	model="${model.id}",
	inputs=${(0, inputs_js_1.getModelInputSnippet)(model)},
	provider="${provider}",
)

print(result)
`,
                },
            ]
            : []),
        {
            client: "requests",
            content: `\
def query(payload):
	response = requests.post(API_URL, headers=headers, json=payload)
	return response.json()
	
output = query({
	"inputs": ${(0, inputs_js_1.getModelInputSnippet)(model)},
})`,
        },
    ];
};
exports.snippetBasic = snippetBasic;
const snippetFile = (model) => {
    return [
        {
            client: "requests",
            content: `\
def query(filename):
	with open(filename, "rb") as f:
		data = f.read()
	response = requests.post(API_URL, headers=headers, data=data)
	return response.json()

output = query(${(0, inputs_js_1.getModelInputSnippet)(model)})`,
        },
    ];
};
exports.snippetFile = snippetFile;
const snippetTextToImage = (model, accessToken, provider) => {
    return [
        {
            client: "huggingface_hub",
            content: `\
${snippetImportInferenceClient(accessToken, provider)}

# output is a PIL.Image object
image = client.text_to_image(
	${(0, inputs_js_1.getModelInputSnippet)(model)},
	model="${model.id}"
)`,
        },
        ...(provider === "fal-ai"
            ? [
                {
                    client: "fal-client",
                    content: `\
import fal_client

result = fal_client.subscribe(
	# replace with correct id from fal.ai
	"fal-ai/${model.id}",
	arguments={
		"prompt": ${(0, inputs_js_1.getModelInputSnippet)(model)},
	},
)
print(result)
`,
                },
            ]
            : []),
        ...(provider === "hf-inference"
            ? [
                {
                    client: "requests",
                    content: `\
def query(payload):
	response = requests.post(API_URL, headers=headers, json=payload)
	return response.content

image_bytes = query({
	"inputs": ${(0, inputs_js_1.getModelInputSnippet)(model)},
})

# You can access the image with PIL.Image for example
import io
from PIL import Image
image = Image.open(io.BytesIO(image_bytes))`,
                },
            ]
            : []),
    ];
};
exports.snippetTextToImage = snippetTextToImage;
const snippetTabular = (model) => {
    return [
        {
            client: "requests",
            content: `\
def query(payload):
	response = requests.post(API_URL, headers=headers, json=payload)
	return response.content

response = query({
	"inputs": {"data": ${(0, inputs_js_1.getModelInputSnippet)(model)}},
})`,
        },
    ];
};
exports.snippetTabular = snippetTabular;
const snippetTextToAudio = (model) => {
    // Transformers TTS pipeline and api-inference-community (AIC) pipeline outputs are diverged
    // with the latest update to inference-api (IA).
    // Transformers IA returns a byte object (wav file), whereas AIC returns wav and sampling_rate.
    if (model.library_name === "transformers") {
        return [
            {
                client: "requests",
                content: `\
def query(payload):
	response = requests.post(API_URL, headers=headers, json=payload)
	return response.content

audio_bytes = query({
	"inputs": ${(0, inputs_js_1.getModelInputSnippet)(model)},
})
# You can access the audio with IPython.display for example
from IPython.display import Audio
Audio(audio_bytes)`,
            },
        ];
    }
    else {
        return [
            {
                client: "requests",
                content: `\
def query(payload):
	response = requests.post(API_URL, headers=headers, json=payload)
	return response.json()
	
audio, sampling_rate = query({
	"inputs": ${(0, inputs_js_1.getModelInputSnippet)(model)},
})
# You can access the audio with IPython.display for example
from IPython.display import Audio
Audio(audio, rate=sampling_rate)`,
            },
        ];
    }
};
exports.snippetTextToAudio = snippetTextToAudio;
const snippetDocumentQuestionAnswering = (model) => {
    return [
        {
            client: "requests",
            content: `\
def query(payload):
	with open(payload["image"], "rb") as f:
		img = f.read()
		payload["image"] = base64.b64encode(img).decode("utf-8")  
	response = requests.post(API_URL, headers=headers, json=payload)
	return response.json()

output = query({
    "inputs": ${(0, inputs_js_1.getModelInputSnippet)(model)},
})`,
        },
    ];
};
exports.snippetDocumentQuestionAnswering = snippetDocumentQuestionAnswering;
exports.pythonSnippets = {
    // Same order as in tasks/src/pipelines.ts
    "text-classification": exports.snippetBasic,
    "token-classification": exports.snippetBasic,
    "table-question-answering": exports.snippetBasic,
    "question-answering": exports.snippetBasic,
    "zero-shot-classification": exports.snippetZeroShotClassification,
    translation: exports.snippetBasic,
    summarization: exports.snippetBasic,
    "feature-extraction": exports.snippetBasic,
    "text-generation": exports.snippetBasic,
    "text2text-generation": exports.snippetBasic,
    "image-text-to-text": exports.snippetConversational,
    "fill-mask": exports.snippetBasic,
    "sentence-similarity": exports.snippetBasic,
    "automatic-speech-recognition": exports.snippetFile,
    "text-to-image": exports.snippetTextToImage,
    "text-to-speech": exports.snippetTextToAudio,
    "text-to-audio": exports.snippetTextToAudio,
    "audio-to-audio": exports.snippetFile,
    "audio-classification": exports.snippetFile,
    "image-classification": exports.snippetFile,
    "tabular-regression": exports.snippetTabular,
    "tabular-classification": exports.snippetTabular,
    "object-detection": exports.snippetFile,
    "image-segmentation": exports.snippetFile,
    "document-question-answering": exports.snippetDocumentQuestionAnswering,
    "image-to-text": exports.snippetFile,
    "zero-shot-image-classification": exports.snippetZeroShotImageClassification,
};
function getPythonInferenceSnippet(model, accessToken, provider, opts) {
    if (model.tags.includes("conversational")) {
        // Conversational model detected, so we display a code snippet that features the Messages API
        return (0, exports.snippetConversational)(model, accessToken, provider, opts);
    }
    else {
        const snippets = model.pipeline_tag && model.pipeline_tag in exports.pythonSnippets
            ? exports.pythonSnippets[model.pipeline_tag]?.(model, accessToken, provider) ?? []
            : [];
        return snippets.map((snippet) => {
            return {
                ...snippet,
                content: snippet.client === "requests"
                    ? `\
import requests

API_URL = "${(0, inference_providers_js_1.openAIbaseUrl)(provider)}"
headers = {"Authorization": ${accessToken ? `"Bearer ${accessToken}"` : `f"Bearer {API_TOKEN}"`}}

${snippet.content}`
                    : snippet.content,
            };
        });
    }
}
